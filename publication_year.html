<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
        
        <meta name="keywords" content="" />
        
        <!-- Page Title -->
        <title>Publications | Juanxi Tian</title>

        <!-- Favicons -->
        <link rel="apple-touch-icon" sizes="180x180" href="assets/img/favicons/icon.svg">
        <link rel="icon" type="image/svg" sizes="32x32" href="assets/img/favicons/icon.svg">
        <link rel="icon" type="image/svg" sizes="16x16" href="assets/img/favicons/icon.svg">
        <link rel="manifest" href="assets/img/favicons/site.webmanifest">
        <link rel="mask-icon" href="https://github.com/tianshijing/tianshijing.github.io/blob/main/assets/img/favicons/icon.svg" color="#f23838">
        <meta name="msapplication-TileColor" content="#da532c">
        <meta name="theme-color" content="#ffffff">

        <!-- Vendor Stylesheets -->
        <link href="https://fonts.googleapis.com/css?family=Oswald:300,400,500,700%7CRoboto:300,400,700" rel="stylesheet">
        <link href="assets/vendor/material-design-iconic-font/dist/css/material-design-iconic-font.min.css" rel="stylesheet">
        <link href="assets/vendor/jquery.mb.vimeo_player/dist/css/jquery.mb.vimeo_player.min.css" rel="stylesheet">
        <link href="assets/vendor/@fortawesome/fontawesome-free/css/all.css" rel="stylesheet">

        <!-- Theme Stylesheets -->
        <link href="assets/css/theme.css" rel="stylesheet">

        <!-- Global site tag (gtag.js) - Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-22940424-1"></script>
        <script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());

            gtag('config', 'UA-22940424-1');
        </script>
        <style>
          a
          {
              text-decoration: none;

              color: #6c757d;
              background-color: transparent;
          }
          a:hover
          {
              text-decoration: underline; 

              color: #c00000;
          }

          a:not([href]):not([class])
          {
              text-decoration: none; 

              color: inherit;
          }
          a:not([href]):not([class]):hover
          {
              text-decoration: none; 

              color: inherit;
          }
          body {
            counter-reset: conference 31 journal 9 workshop 11 informal 15;
          }
        </style>
    </head>

    <body>
        <!-- Preloader -->
        <div class="preloader">
            <div class="spinner">
                <div class="circles"></div>
            </div>
        </div>
        <!-- End of Preloader -->

        <!-- Header -->
        <header class="spyre-navbar navbar navbar-expand-lg bg-dark navbar-dark fixed-top align-items-center" data-transparent data-text-color="#ffffff">
            <div class="container">
              <a class="navbar-brand mr-lg-5 mr-xl-7 mt-auto" href="index.html">
                <img src="assets/img/favicons/logo.svg" class="d-none d-lg-block" alt="logo" width="183" />
                <img src="assets/img/favicons/logo.svg" class="d-block d-lg-none" alt="logo" width="150" />
              </a>

                <div class="collapse navbar-collapse" id="navbarSupportedContent">
                    <ul class="navbar-nav mr-auto">
                        <li class="pl-2 nav-item navbar-text"><a href="index.html" class="nav-link text-400">Home</a></li>
                        <!-- <li class="pl-5 nav-item navbar-text"><a href="team.html" class="nav-link text-400">Team</a></li> -->
                        <li class="pl-5 nav-item navbar-text"><a href="research.html" class="nav-link text-400">Research</a></li>
                        <li class="pl-5 nav-item navbar-text"><a href="research.html#overview" class="nav-link">Publications</a></li>
                        <!-- <li class="pl-5 nav-item navbar-text"><a href="careers.html" class="nav-link text-400">Join Us</a></li> -->
                    </ul>
                </div>

                <div class="menu-toggle d-block d-lg-none">
                    <div class="hamburger">
                        <span></span>
                        <span></span>
                        <span></span>
                    </div>
                    <div class="cross">
                        <span></span>
                        <span></span>
                    </div>
                </div>
            </div>

            <!-- Spyrenav Overlay -->
            <div class="spyre-navbar-overlay overlay-slide">
                <div class="container">
                    <div class="row">
                        <div class="spyre-navbar-nav-container col-md-6 col-lg-5 col-xl-4 bg-white ext-l">
                            <nav class="spyre-navbar-nav">
                                <ul class="spyre-nav">
                                    <li class="spyre-nav-item"><a href="index.html" class="spyre-nav-link">Home</a></li>
                                    <li class="spyre-nav-item"><a href="research.html" class="spyre-nav-link">Our Research</a></li>
                                    <!-- <li class="spyre-nav-item"><a href="team.html" class="spyre-nav-link">Team</a></li> -->
                                    <li class="spyre-nav-item dropdown">
                                        <a href="publication_year.html#" class="spyre-nav-link dropdown-toggle" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Publications</a>
                                        <ul class="dropdown-menu" aria-labelledby="navbarDropdown">
                                            <li class="dropdown-menu-item"><a href="research.html#overview" class="dropdown-menu-link">By Topic</a></li>
                                            <li class="dropdown-menu-item"><a href="publication_year.html" class="dropdown-menu-link">By Year</a></li>
                                        </ul>
                                    </li>
                                </ul>
                            </nav>
                        </div>
        
                        <div class="col-lg-7 col-xl-8 d-none d-md-block">
                            <div class="d-flex flex-column h-100">
                                <div class="d-flex h-100">
                                    <div class="align-self-center">
                                        <div class="text-uppercase"
                                            data-background-text="computer vision"
                                            data-color="#7079a2"
                                            data-opacity="0.02"
                                            data-font-size="85px"
                                            data-font-weight="500"
                                            data-offset-x="-5%"
                                            data-letter-spacing="5px"
                                        ></div>
                                        <div class="text-uppercase"
                                            data-background-text="mmlab"
                                            data-color="#7079a2"
                                            data-opacity="0.04"
                                            data-font-size="175px"
                                            data-font-weight="500"
                                            data-offset-x="29%"
                                            data-padding="7vh 0 2vh 0"
                                            data-letter-spacing="5px"
                                        ></div>
                                        <div class="text-uppercase"
                                            data-background-text="deep learning"
                                            data-color="#7079a2"
                                            data-opacity="0.03"
                                            data-font-size="140px"
                                            data-font-weight="500"
                                            data-offset-x="15%"
                                            data-letter-spacing="5px"
                                        ></div>
                                    </div>
                                </div>
                                
                                <div class="mt-auto">
                                    <ul class="nav flex-nowrap float-right">
                                        <li class="nav-item">
                                            <a class="nav-link px-2" href="https://x.com/JuanxiTian" target="_blank">
                                                <i class="zmdi zmdi-twitter text-white"></i>
                                            </a>
                                        </li>
                                    </ul>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            <!-- End of Spyrenav Overlay -->
        </header>
        <!-- End of Header -->

        <!-- Main Content -->
        <main class="main minh-100vh">
            <!-- Section -->
            <section class="py-0 overflow-hidden text-center">
                <div class="bg-container overlay overlay-dark-60 parallax" data-rellax-percentage="0.5" style="background-image: url(assets/img/backgrounds/bg-03.jpg)">
                </div>
            </section>
            <!-- End of Section -->

            <!-- Section -->
            <section id="section-1" class="pb-0">
                <div class="container">
                    <div class="row">
                        <div class="col-lg-4 order-lg-1">
                            <div class="pb-6 pt-6 py-lg-3" data-toggle="sticky" data-sticky-offset-top="100">
                                <h5 class="mb-4 text-uppercase text-600">Years</h5>
                                <ul class="mb-5 mb-lg-6 pl-4 text-600">
                                    <li class="mb-1"><a href="publication_year.html#2025" class="text-600">2025</a></li>
                                    <li class="mb-1"><a href="publication_year.html#2024" class="text-600">2024</a></li>
                                </ul>

                                <h5 class="mb-4 text-uppercase text-600">Sort by</h5>
                                <ul class="mb-5 mb-lg-6 pl-4 text-600">
                                    <li class="mb-1"><a href="research.html#overview" class="text-600">Topics</a></li>
                                </ul>
                                
                                <!-- <h5 class="mb-4 text-uppercase text-600">Highlights</h5>
                                <ul class="mb-5 mb-lg-6 pl-4 text-600">
                                    <li class="mb-1"><a href="conference/cvpr2023/index.html" class="text-600">CVPR 2023</a></li>
                                </ul> -->
                            </div>
                        </div>


                        <div class="col-lg-8 order-lg-2 pb-6 pb-lg-0 pt-lg-3">
                            <div class="mb-8">
                              <h6>* Equal contribution.&nbsp;&nbsp; <sup>&dagger;</sup> Project lead.&nbsp;&nbsp; <sup>&Dagger;</sup> Corresponding author.</h6>
                              <span id="2025"></span>

                            </div>

                            <div class="mb-8">
                              <h2 class="mb-4 text-uppercase">2025</h2>
                              <h4 class="mb-4 text-uppercase">Conference</h4>
                              <ol class="conference">
                                <li>
                                  <span class="text-primary"><b>AuroraCap: Efficient, Performant Video Detailed Captioning and a New Benchmark</b></span> 
                                  <br />
                                  <span class="text-500">
                                  <u>Wenhao Chai</u>*<sup>&dagger;</sup>, Enxin Song*, Yilun Du, Chenlin Meng, Vashisht Madhavan, Omer Bar-Tal, Jenq-Neng Hwang, Saining Xie, Christopher D. Manning <br /> 
                                </span>
                                <span class="text-900"> 
                                  International Conference on Learning Representations (ICLR), 2025<br />   
                                  </span>
                                  <a href="https://rese1f.github.io/aurora-web/">Project Page</a>
                                  |
                                  <a href="https://arxiv.org/abs/2410.03051">Paper</a>
                                  |
                                  <a href="https://www.youtube.com/watch?v=ePpGJDR1FxM">Video</a>
                                  |
                                  <a href="https://huggingface.co/collections/wchai/auroracap-66d117ffe13bedda96702013">Model</a>
                                  |
                                  <a href="https://huggingface.co/datasets/wchai/Video-Detailed-Caption">Benchmark</a>
                                  |
                                  <a href="https://rese1f.github.io/aurora-web/">Leaderboard</a>
                                  |
                                  <!-- <a href="https://huggingface.co/datasets/wchai/AuroraCap-trainset">Dataset</a>
                                  | -->
                                  <a href="assets/file/poster/auroracap_iclr25.pdf">Poster</a>
                                  |
                                  <a href="https://github.com/rese1f/aurora">Code</a>
                                </li>
                                <hr>
                                <li>
                                  <span class="text-primary"><b>PAD: Personalized Alignment at Decoding-Time</b></span> 
                                  <br />
                                  <span class="text-500">
                                    Ruizhe Chen*, Xiaotian Zhang*, Meng Luo*, <b>Wenhao Chai*</b>, Zuozhu Liu<sup>&Dagger;</sup> <br /> 
                                  </span>
                                  <span class="text-900">  
                                    International Conference on Learning Representations (ICLR), 2025<br />  
                                  </span>
                                  <a href="https://arxiv.org/abs/2410.04070">Paper</a>
                                  |
                                  <a href="assets/file/poster/pad_iclr25.pdf">Poster</a>
                                </li>
                                <hr>
                                <li>
                                  <span class="text-primary"><b>Zero-shot 3D Question Answering via Voxel-based Dynamic Token Compressiond</b></span> 
                                  <br />
                                  <span class="text-500">
                                  Hsiang-Wei Huang, Fu-Chen Chen, <b>Wenhao Chai</b>, Che-Chun Su, Lu Xia, Sanghun Jung, Cheng-Yen Yang, Jenq-Neng Hwang, Min Sun, Cheng-Hao Kuo<sup>&Dagger;</sup> <br /> 
                                  </span>
                                  <span class="text-900">  
                                    Computer Vision and Pattern Recognition (CVPR), 2025 <br /> 
                                  </span>
                                  <a href="https://www.amazon.science/publications/zero-shot-3d-question-answering-via-voxel-based-dynamic-token-compression">Paper</a>
                                  |
                                  <a href="assets/file/poster/dtc_cvpr25.pdf">Poster</a>
                                </li>
                                <hr>
                                <li>
                                  <span class="text-primary"><b>Science-T2I: Addressing Scientific Illusions in Image Synthesis</b></span> 
                                  <br />
                                  <span class="text-500">
                                  Jialuo Li, <b>Wenhao Chai</b>, Xingyu Fu, Haiyang Xu, Saining Xie<sup>&Dagger;</sup> <br />
                                  </span>
                                  <span class="text-900"> 
                                    Computer Vision and Pattern Recognition (CVPR), 2025 <br /> 
                                  </span>
                                  <a href="https://jialuo-li.github.io/Science-T2I-Web/">Project Page</a>
                                  |
                                  <a href="https://arxiv.org/abs/2504.13129">Paper</a>
                                  |
                                  <a href="https://github.com/Jialuo-Li/Science-T2I">Code</a>
                                </li>
                                <hr>
                                <li>
                                  <span class="text-primary"><b>MonoTAKD: Teaching Assistant Knowledge Distillation for Monocular 3D Object Detection</b></span> 
                                  <br />
                                  <span class="text-500">
                                    Hou-I Liu, Christine Wu, Jen-Hao Cheng, <b>Wenhao Chai</b>, Shian-Yun Wang, Gaowen Liu, Jenq-Neng Hwang, Hong-Han Shuai, Wen-Huang Cheng<sup>&Dagger;</sup> <br /> 
                                  </span>
                                  <span class="text-900">  
                                    Computer Vision and Pattern Recognition (CVPR), 2025<br /> 
                                  </span>
                                  <a href="https://arxiv.org/abs/2404.04910">Paper</a>
                                </li>
                                <hr>
                                <li>
                                  <span class="text-primary"><b>DiffPO: Diffusion-styled Preference Optimization for Efficient Inference-Time Alignment of Large Language Models</b></span> 
                                  <br />
                                  <span class="text-500">
                                    Ruizhe Chen, <b>Wenhao Chai</b>, Zhifei Yang, Xiaotian Zhang, Joey Tianyi Zhou, Tony Quek, Soujanya Poria, Zuozhu Liu<sup>&Dagger;</sup> <br /> 
                                  </span>
                                  <span class="text-900">  
                                    Annual Meeting of the Association for Computational Linguistics (ACL), 2025<br /> 
                                  </span>
                                  <a href="https://arxiv.org/abs/2503.04240">Paper</a>
                                  |
                                  <a href="https://github.com/zjuruizhechen/DiffPO">Code</a>
                                </li>
                                <hr>
                                <li>
                                  <span class="text-primary"><b>PromptHaze: Prompting Real-world Dehazing via Depth Anything Model</b></span> 
                                  <br />
                                  <span class="text-500">
                                    Tian Ye, Sixiang Chen, Haoyu Chen, <b>Wenhao Chai</b>, Jingjing Ren, Zhaohu Xing, Wenxue Li, Lei Zhu<sup>&Dagger;</sup> <br /> 
                                  </span>
                                  <span class="text-900">  
                                    Association for the Advancement of Artificial Intelligence (AAAI), 2025<br /> 
                                  </span>
                                  <a href="https://ojs.aaai.org/index.php/AAAI/article/view/33024">Paper</a>
                                </li>
                                <hr>
                                <li>
                                  <span class="text-primary"><b>AGLLDiff: Guiding Diffusion Models Towards Unsupervised Training-free Real-world Low-light Image Enhancement</b></span> 
                                  <br />
                                  <span class="text-500">
                                    Yunlong Lin, Tian Ye, Sixiang Chen, Zhenqi Fu, Yingying Wang, <b>Wenhao Chai</b>, Zhaohu Xing, Lei Zhu, Xinghao Ding<sup>&Dagger;</sup> <br /> 
                                  </span>
                                  <span class="text-900">  
                                    Association for the Advancement of Artificial Intelligence (AAAI), 2025<br /> 
                                  </span>
                                  <a href="https://aglldiff.github.io/">Project Page</a>
                                  |
                                  <a href="https://arxiv.org/abs/2407.14900">Paper</a>
                                </li>
                                <hr>
                                <li>
                                  <span class="text-primary"><b>MambaMOT: State-Space Model as Motion Predictor for Multi-Object Tracking</b></span> 
                                  <br />
                                  <span class="text-500">
                                    Hsiang-Wei Huang, Cheng-Yen Yang, <b>Wenhao Chai</b>, Zhongyu Jiang, Jenq-Neng Hwang<sup>&Dagger;</sup> <br /> 
                                  </span>
                                  <span class="text-900">
                                    International Conference on Acoustics, Speech, and Signal Processing (ICASSP) Oral, 2025<br /> 
                                  </span>
                                  <a href="https://arxiv.org/abs/2403.10826">Paper</a>
                                </li>
                                <hr>
                                <li>
                                  <span class="text-primary"><b>UniHPE: Towards Unified Human Pose Estimation via Contrastive Learning</b></span> 
                                  <br />
                                  <span class="text-500">
                                    Zhongyu Jiang, <b>Wenhao Chai</b>, Lei Li, Zhuoran Zhou, Cheng-Yen Yang, Jenq-Neng Hwang<sup>&Dagger;</sup> <br /> 
                                  </span>
                                  <span class="text-900">
                                    IEEE International Conference on Multimedia Information Processing and Retrieval, 2025<br /> 
                                  </span>
                                  <a href="https://arxiv.org/abs/2311.16477">Paper</a>
                                </li>

                              <br>
                              </ol>
                              <h4 class="mb-4 text-uppercase">Journal</h4>
                              <ol class="journal">
                                <li>
                                  <span class="text-primary"><b>Efficient Transfer from Image-based Large Multimodal Models to Video Tasks</b></span> 
                                  <br />
                                  <span class="text-500">
                                    Shidong Cao, Zhonghan Zhao, Shengyu Hao, <b>Wenhao Chai</b>, Jenq-Neng Hwang, Hongwei Wang, Gaoang Wang<sup>&Dagger;</sup> <br /> 
                                  </span>
                                  <span class="text-900">  
                                    IEEE Transactions on Multimedia (TMM) <br /> 
                                  </span>
                                  <a href="https://ieeexplore.ieee.org/abstract/document/10948327">Paper</a>
                                </li>
                                <hr>
                                <li>
                                  <span class="text-primary"><b>A Survey of Deep Learning in Sports Applications: Perception, Comprehension, and Decision</b></span> 
                                  <br />
                                  <span class="text-500">
                                    Zhonghan Zhao, <b>Wenhao Chai</b>, Shengyu Hao, Wenhao Hu, Guanhong Wang, Shidong Cao, Gaoang Wang<sup>&Dagger;</sup>, Mingli Song, Jenq-Neng Hwang <br /> 
                                  </span>
                                  <span class="text-900">
                                    IEEE Transactions on Visualization and Computer Graphics (TVCG) <br /> 
                                  </span>
                                  <a href="https://arxiv.org/abs/2307.03353">Paper</a>
                                </li>
                                <hr>
                                <li>
                                  <span class="text-primary"><b>Pose-Guided Transformer for Fine-Grained Action Quality Assessment</b></span> 
                                  <br />
                                  <span class="text-500">
                                    Yanting Zhang, Xia Li, <b>Wenhao Chai</b>, Cairong Yan, Wenhai Wang, Gaoang Wang<br /> 
                                  </span>
                                  <span class="text-900">  
                                    IEEE Transactions on Circuits and Systems for Video Technology (TCSVT) <br /> 
                                  </span>
                                  <a href="https://ieeexplore.ieee.org/abstract/document/10902635">Paper</a>
                                </li>
                              </ol>
                              <br>

                              <h4 class="mb-4 text-uppercase">Workshop</h4>
                                <ol class="workshop">

                                  <li>
                                    <span class="text-primary"><b>Bringing RNNs Back to Efficient Open-Ended Video Understanding</b></span> 
                                    <br />
                                    <span class="text-500">
                                      Weili Xu, Enxin Song, <b>Wenhao Chai</b>, Tian Ye, Gaoang Wang<br /> 
                                    </span>
                                    <span class="text-900">  
                                      Computer Vision and Pattern Recognition (CVPR) Workshop @ Efficient Large Vision Models, 2025 <br /> 
                                    </span>
                                    <a href="publication_year.html">Paper</a>
                                  </li>
                                  <hr>
                                  <li>
                                    <span class="text-primary"><b>3D Visual Grounding with Reasoning LLM</b></span> 
                                    <br />
                                    <span class="text-500">
                                      Hsiang-Wei Huang, Kuang-Ming Chen, <b>Wenhao Chai</b>, Cheng-Yen Yang, Jen-Hao Cheng, Jenq-Neng Hwang<br /> 
                                    </span>
                                    <span class="text-900">  
                                      Computer Vision and Pattern Recognition (CVPR) Workshop @ 3D LLM/VLA, 2025 <br /> 
                                    </span>
                                    <a href="publication_year.html">Paper</a>
                                  </li>
                                  <hr>
                                  <li>
                                    <span class="text-primary"><b>ToSA: Token Merging with Spatial Awareness</b></span> 
                                    <br />
                                    <span class="text-500">
                                      Hsiang-Wei Huang, <b>Wenhao Chai</b>, Kuang-Ming Chen, Cheng-Yen Yang, Jenq-Neng Hwang<br /> 
                                    </span>
                                    <span class="text-900">  
                                      Computer Vision and Pattern Recognition (CVPR) Workshop @ 3D LLM/VLA, 2025 <br /> 
                                    </span>
                                    <a href="publication_year.html">Paper</a>
                                  </li>
                                  <hr>
                                  <li>
                                    <span class="text-primary"><b>CityGen: Infinite and Controllable 3D City Layout Generation</b></span> 
                                    <br />
                                    <span class="text-500">
                                      Jie Deng, <b>Wenhao Chai</b>, Jianshu Guo, Qixuan Huang, Wenhao Hu, Jenq-Neng Hwang, Gaoang Wang<sup>&Dagger;</sup> <br /> 
                                    </span>
                                    <span class="text-900">
                                      Computer Vision and Pattern Recognition (CVPR) Workshop @ Urban Scene Modeling, 2025<br /> 
                                    </span>
                                    <a href="https://rese1f.github.io/CityGen/">Project Page</a>
                                    |
                                    <a href="https://arxiv.org/abs/2312.01508">Paper</a>
                                    |
                                    <a href="https://github.com/rese1f/CityGen">Code</a>
                                  </li>


                              </ol>
                              <br>
                              <h4 class="mb-4 text-uppercase">Preprint</h4>
                                <ol class="informal">
                                  <li>
                                    <span class="text-primary"><b>Video-MMLU: A Massive Multi-Discipline Lecture Understanding Benchmark</b></span> 
                                    <br />
                                    <span class="text-500">
                                      Enxin Song, <b>Wenhao Chai</b>, Weili Xu, Jianwen Xie, Yuxuan Liu, Gaoang Wang<sup>&Dagger;</sup> <br /> 
                                    </span>
                                    <span class="text-900">  
                                      arXiv Preprint.<br /> 
                                    </span>
                                    <a href="https://enxinsong.com/Video-MMLU-web/">Project Page</a>
                                    |
                                    <a href="https://arxiv.org/abs/2504.14693">Paper</a>
                                    |
                                    <a href="https://github.com/Espere-1119-Song/Video-MMLU">Code</a>
                                    |
                                    <a href="https://huggingface.co/datasets/Enxin/Video-MMLU">Data</a>
                                  </li>
                                  <hr>
                                  <li>
                                    <span class="text-primary"><b>Multimodal Representation Alignment for Image Generation: Text-Image Interleaved Control Is Easier Than You Think</b></span> 
                                    <br />
                                    <span class="text-500">
                                      Liang Chen, Shuai Bai, <b>Wenhao Chai</b>, Weichu Xie, Haozhe Zhao, Leon Vinci, Junyang Lin, Baobao Chang<sup>&Dagger;</sup> <br /> 
                                    </span>
                                    <span class="text-900">  
                                      arXiv Preprint.<br /> 
                                    </span>
                                    <a href="https://arxiv.org/abs/2502.20172">Paper</a>
                                    |
                                    <a href="https://github.com/chenllliang/DreamEngine">Code</a>
                                    |
                                    <a href="https://huggingface.co/leonardPKU/DreamEngine-ObjectFusion/tree/main">Model</a>
                                  </li>
                                  <hr>
                                  <li>
                                    <span class="text-primary"><b>Envisioning Beyond the Pixels: Benchmarking Reasoning-Informed Visual Editing</b></span> 
                                    <br />
                                    <span class="text-500">
                                      Xiangyu Zhao, Peiyuan Zhang, Kexian Tang, Hao Li, <b>Wenhao Chai</b>, Zicheng Zhang, Guangtao Zhai, Junchi Yan, Hua Yang, Xue Yang, Haodong Duan<sup>&Dagger;</sup> <br /> 
                                    </span>
                                    <span class="text-900">  
                                      arXiv Preprint.<br /> 
                                    </span>
                                    <a href="https://arxiv.org/abs/2504.02826">Paper</a>
                                    |
                                    <a href="https://github.com/PhoenixZ810/RISEBench">Code</a>
                                    |
                                    <a href="https://huggingface.co/datasets/PhoenixZ/RISEBench">Dataset</a>
                                  </li>
                                  <hr>
                                  <li>
                                    <span class="text-primary"><b>TEMPURA: Temporal Event Masked Prediction and Understanding for Reasoning in Action</b></span> 
                                    <br />
                                    <span class="text-500">
                                      Jen-Hao Cheng, Vivian Wang, Huayu Wang, Huapeng Zhou, Yi-Hao Peng, Hou-I Liu, Hsiang-Wei Huang, Kuang-Ming Chen, Cheng-Yen Yang, <b>Wenhao Chai</b>, Yi-Ling Chen, Vibhav Vineet, Qin Cai, Jenq-Neng Hwang<sup>&Dagger;</sup> <br /> 
                                    </span>
                                    <span class="text-900">  
                                      arXiv Preprint.<br /> 
                                    </span>
                                    <a href="https://andy-cheng.github.io/TEMPURA/">Project Page</a>
                                    |
                                    <a href="https://arxiv.org/abs/2505.01583">Paper</a>
                                    |
                                    <a href="https://github.com/Andy-Cheng/TEMPURA">Code</a>
                                    |
                                    <a href="https://huggingface.co/datasets/andaba/TEMPURA-VER">Dataset</a>
                                    |
                                    <a href="https://huggingface.co/andaba/TEMPURA-Qwen2.5-VL-3B-s2">Model</a>
                                  </li>
                                  <hr>
                                  <li>
                                    <span class="text-primary"><b>GAM-Agent: Game-Theoretic and Uncertainty-Aware Collaboration for Complex Visual Reasoning</b></span> 
                                    <br />
                                    <span class="text-500">
                                      Jusheng Zhang, Yijia Fan, Wenjun Lin, Ruiqi Chen, Haoyi Jiang, <b>Wenhao Chai</b>, Jian Wang, Keze Wang<sup>&Dagger;</sup> <br /> 
                                    </span>
                                    <span class="text-900">  
                                      arXiv Preprint.<br /> 
                                    </span>
                                    <a href="https://arxiv.org/abs/2505.23399">Paper</a>
                                  </li>
                                  <hr>
                                  <li>
                                    <span class="text-primary"><b>EMMOE: A Comprehensive Benchmark for Embodied Mobile Manipulation in Open Environments</b></span> 
                                    <br />
                                    <span class="text-500">
                                      Dongping Li*, Tielong Cai*, Tianci Tang*, <b>Wenhao Chai</b>, Katherine Rose Driggs-Campbell, Gaoang Wang<sup>&Dagger;</sup> <br /> 
                                    </span>
                                    <span class="text-900">  
                                      arXiv Preprint.<br /> 
                                    </span>
                                    <a href="https://silence143.github.io/emmoe.github.io/">Project Page</a>
                                    |
                                    <a href="https://arxiv.org/abs/2503.08604">Paper</a>
                                    |
                                    <a href="https://github.com/silence143/EMMOE">Code</a>
                                    |
                                    <a href="https://huggingface.co/datasets/Dongping-Li/EMMOE-100">Dataset</a>
                                    |
                                    <a href="https://huggingface.co/collections/Dongping-Li/emmoe-dataset-and-model-67c6b04da2b83b08ec273ef2">Model</a>
                                  </li>
                                  <hr>
                                  <li>
                                    <span class="text-primary"><b>An Empirical Study of GPT-4o Image Generation Capabilities</b></span> 
                                    <br />
                                    <span class="text-500">
                                      Sixiang Chen, Jinbin Bai, Zhuoran Zhao, Tian Ye, Qingyu Shi, Donghao Zhou, <b>Wenhao Chai</b>, Xin Lin, Jianzong Wu, Chao Tang, Shilin Xu, Tao Zhang, Haobo Yuan, Yikang Zhou, Wei Chow, Linfeng Li, Xiangtai Li, Lei Zhu, Lu Qi <br /> 
                                    </span>
                                    <span class="text-900">  
                                      arXiv Preprint.<br /> 
                                    </span>
                                    <a href="https://arxiv.org/abs/2504.05979">Paper</a>
                                  </li>
                                  <hr>
                                  <li>
                                    <span class="text-primary"><b>PackDiT: Joint Human Motion and Text Generation via Mutual Prompting</b></span> 
                                    <br />
                                    <span class="text-500">
                                      Zhongyu Jiang, <b>Wenhao Chai</b>, Zhuoran Zhou, Cheng-Yen Yang, Hsiang-Wei Huang, Jenq-Neng Hwang<sup>&Dagger;</sup> <br /> 
                                    </span>
                                    <span class="text-900">  
                                      arXiv Preprint.<br /> 
                                    </span>
                                    <a href="https://arxiv.org/abs/2501.16551">Paper</a>
                                  </li>
                                  <hr>
                                  <li>
                                    <span class="text-primary"><b>Pointmap Association and Piecewise-Plane Constraint for Consistent and Compact 3D Gaussian Segmentation Field</b></span> 
                                    <br />
                                    <span class="text-500">
                                      Wenhao Hu, <b>Wenhao Chai</b>, Shengyu Hao, Xiaotong Cui, Xuexiang Wen, Jenq-Neng Hwang, Gaoang Wang<sup>&Dagger;</sup> <br /> 
                                    </span>
                                    <span class="text-900">  
                                      arXiv Preprint. <br /> 
                                    </span>
                                    <a href="https://whuechoscript.github.io/CCGS/">Project Page</a>
                                    |
                                    <a href="https://arxiv.org/abs/2502.16303">Paper</a>
                                  </li>
                                </ol>

                                <span id="2024"></span>
                            </div>


                            <div class="mb-8">
                                <h2 class="mb-4 text-uppercase">2024</h2>

                                <h4 class="mb-4 text-uppercase">Conference</h4>
                                <ol class="conference">
                                    <li>
                                      <span class="text-primary"><b>MovieChat: From Dense Token to Sparse Memory in Long Video Understanding</b></span> 
                                      <br />
                                      <span class="text-500">
                                        Enxin Song*, <b>Wenhao Chai</b>*<sup>&dagger;</sup>, Guanhong Wang*, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Haozhe Chi, Xun Guo, Tian Ye, Yanting Zhang, Yan Lu, Jenq-Neng Hwang, Gaoang Wang<sup>&Dagger;</sup> <br /> 
                                      </span>
                                      <span class="text-900">
                                        Computer Vision and Pattern Recognition (CVPR), 2024<br /> 
                                      </span>
                                      <a href="https://rese1f.github.io/MovieChat/">Project Page</a>
                                      |
                                      <a href="https://arxiv.org/abs/2307.16449">Paper</a>
                                      |
                                      <a href="https://mp.weixin.qq.com/s/OGI9kbH80sc9oIWWnntEIA">Blog</a>
                                      |
                                      <a href="https://www.youtube.com/watch?v=rNzCzflVqBU">Video</a>
                                      |
                                      <a href="https://huggingface.co/datasets/Enxin/MovieChat-1K-test">Dataset</a>
                                      |
                                      <a href="https://github.com/rese1f/MovieChat">Leaderboard</a>
                                      |
                                      <a href="https://github.com/rese1f/MovieChat">Code</a>
                                    </li>
                                    <hr>
                                    <li>
                                      <span class="text-primary"><b>Learning Diffusion Texture Priors for Image Restoration</b></span> 
                                      <br />
                                      <span class="text-500">
                                        Tian Ye, Sixiang Chen, <b>Wenhao Chai</b>, Zhaohu Xing, Jing Qin, Ge Lin, Lei Zhu<sup>&Dagger;</sup> <br /> 
                                      </span>
                                      <span class="text-900">
                                        Computer Vision and Pattern Recognition (CVPR) Highlight, 2024 <br /> 
                                      </span>
                                      <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Ye_Learning_Diffusion_Texture_Priors_for_Image_Restoration_CVPR_2024_paper.html">Paper</a>
                                    </li>
                                    <hr>
                                    <li>
                                      <span class="text-primary"><b>See and Think: Embodied Agent in Virtual Environment</b></span> 
                                      <br />
                                      <span class="text-500">
                                        Zhonghan Zhao*, <b>Wenhao Chai</b>*<sup>&dagger;</sup>, Xuan Wang*, Boyi Li, Shengyu Hao, Shidong Cao, Tian Ye, Jenq-Neng Hwang, Gaoang Wang<sup>&Dagger;</sup> <br /> 
                                        </span>
                                        <span class="text-900">
                                        European Conference on Computer Vision (ECCV), 2024<br /> 
                                        </span>
                                        <a href="https://rese1f.github.io/STEVE/">Project Page</a>
                                        |
                                        <a href="https://arxiv.org/abs/2311.15209">Paper</a>
                                        |
                                        <a href="https://github.com/rese1f/STEVE">Dataset</a>
                                        |
                                        <a href="https://github.com/rese1f/STEVE">Code</a>
                                    </li>
                                    <hr>
                                    <li>
                                      <span class="text-primary"><b>RT-Pose: A 4D Radar Tensor-based 3D Human Pose Estimation and Localization Benchmark</b></span> 
                                      <br />
                                      <span class="text-500">
                                      Yuan-Hao Ho, Jen-Hao Cheng, Sheng Yao Kuan, Zhongyu Jiang, <b>Wenhao Chai</b>, Hsiang-Wei Huang, Jenq-Neng Hwang, Chih-Lung Lin<sup>&Dagger;</sup> <br /> 
                                    </span>
                                    <span class="text-900">
                                      European Conference on Computer Vision (ECCV), 2024<br /> 
                                      </span>
                                      <a href="https://arxiv.org/abs/2407.13930">Paper</a>
                                      |
                                      <a href="https://huggingface.co/datasets/uwipl/RT-Pose">Dataset</a>
                                      |
                                      <a href="https://github.com/ipl-uw/RT-POSE">Code</a>
                                    </li>
                                    <hr>
                                    <li>
                                        <span class="text-primary"><b>UniAP: Towards Universal Animal Perception in Vision via Few-shot Learning</b></span> 
                                        <br />
                                        <span class="text-500">
                                        Meiqi Sun*, Zhonghan Zhao*, <b>Wenhao Chai</b>*, Hanjun Luo, Shidong Cao, Yanting Zhang, Jenq-Neng Hwang, Gaoang Wang<sup>&Dagger;</sup> <br /> 
                                      </span>
                                      <span class="text-900">
                                        Association for the Advancement of Artificial Intelligence (AAAI), 2024<br /> 
                                        </span>
                                        <a href="https://rese1f.github.io/UniAP/">Project Page</a>
                                        |
                                        <a href="https://arxiv.org/abs/2308.09953">Paper</a>
                                        |
                                        <a href="https://github.com/rese1f/UniAP">Code</a>
                                    </li>
                                    <hr>
                                    <li>
                                      <span class="text-primary"><b>Ego3DT: Tracking Every 3D Object in Ego-centric Videos</b></span>
                                      <br />
                                      <span class="text-500">
                                        Shengyu Hao*, <b>Wenhao Chai</b>*, Zhonghan Zhao*, Meiqi Sun, Wendi Hu, Jieyang Zhou, Yixian Zhao, Qi Li, Yizhou Wang, Xi Li, Gaoang Wang<sup>&Dagger;</sup> <br />
                                      </span>
                                      <span class="text-900">
                                        ACM International Conference on Multimedia (ACM MM), 2024<br />
                                      </span>
                                      <a href="https://arxiv.org/abs/2410.08530">Paper</a>
                                    </li>
                                    <hr>
                                    <li>
                                      <span class="text-primary"><b>LLaVA-Ultra: Large Chinese Language and Vision Assistant for Ultrasound</b></span>
                                      <br />
                                      <span class="text-500">
                                        Xuechen Guo, <b>Wenhao Chai</b>, Shi-Yan Li, Gaoang Wang<sup>&Dagger;</sup> <br />
                                      </span>
                                      <span class="text-900">
                                        ACM International Conference on Multimedia (ACM MM), 2024<br />
                                      </span>
                                      <a href="https://openreview.net/forum?id=7ZYEoB71Vd">Paper</a>
                                    </li>
                                    <hr>
                                    <li>
                                        <span class="text-primary"><b>Blind Inpainting with Object-aware Discrimination for Artificial Marker Removal</b></span>
                                        <br />
                                        <span class="text-500">
                                          Xuechen Guo, Wenhao Hu, Chiming Ni, <b>Wenhao Chai</b>, Shiyan Li, Gaoang Wang<sup>&Dagger;</sup> <br />
                                        </span>
                                        <span class="text-900">
                                          International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2024<br />
                                        </span>
                                        <a href="https://arxiv.org/abs/2303.15124">Paper</a>
                                    </li>
                                    <hr>
                                    <li>
                                        <span class="text-primary"><b>Back to Optimization: Diffusion-based Zero-Shot 3D Human Pose Estimation</b></span> 
                                        <br />
                                        <span class="text-500">
                                          Zhongyu Jiang, Zhuoran Zhou, Lei Li, <b>Wenhao Chai</b>, Cheng-Yen Yang, Jenq-Neng Hwang<sup>&Dagger;</sup> <br /> 
                                        </span>
                                        <span class="text-900">
                                          Winter Conference on Applications of Computer Vision (WACV), 2024<br /> 
                                        </span>
                                        <a href="https://zhyjiang.github.io/ZeDO-proj/">Project Page</a>
                                        |
                                        <a href="https://arxiv.org/abs/2307.03833">Paper</a>
                                        |
                                        <a href="https://github.com/ipl-uw/ZeDO-Release">Code</a>
                                      </li>
                                      <hr>
                                      <li>
                                        <span class="text-primary"><b>MPM: A Unified 2D-3D Human Pose Representation via Masked Pose Modeling</b></span> 
                                        <br />
                                        <span class="text-500">
                                          Zhenyu Zhang, <b>Wenhao Chai</b>, Zhongyu Jiang, Tian Ye, Mingli Song, Jenq-Neng Hwang, Gaoang Wang<sup>&Dagger;</sup> <br /> 
                                        </span>
                                        <span class="text-900">
                                          Chinese Conference on Pattern Recognition and Computer Vision (PRCV), 2024<br /> 
                                        </span>
                                        <a href="https://arxiv.org/abs/2306.17201">Paper</a>
                                        |
                                        <a href="https://github.com/vvirgooo2/MPM">Code</a>
                                      </li>
                                      <hr>
                                      <li>
                                        <span class="text-primary"><b>Chasing Consistency in Text-to-3D Generation from a Single Image</b></span> 
                                        <br />
                                        <span class="text-500">
                                          Yichen Ouyang, <b>Wenhao Chai</b>, Jiayi Ye, Dapeng Tao, Yibing Zhan, Gaoang Wang<sup>&Dagger;</sup> <br /> 
                                        </span>
                                        <span class="text-900">
                                          ACM International Conference on Multimedia in Asia (ACM MM Asia), 2024<br /> 
                                        </span>
                                        <a href="https://arxiv.org/abs/2309.03599">Paper</a>
                                      </li>
                                      <hr>
                                      <li>
                                        <span class="text-primary"><b>Boosting Online 3D Multi-Object Tracking through Camera-Radar Cross Check</b></span> 
                                        <br />
                                        <span class="text-500">
                                          Sheng-Yao Kuan, Jen-Hao Cheng, Hsiang-Wei Huang, <b>Wenhao Chai</b>, Cheng-Yen Yang, Hugo Latapie, Gaowen Liu, Bing-Fei Wu, Jenq-Neng Hwang<sup>&Dagger;</sup> <br /> 
                                        </span>
                                        <span class="text-900">
                                          Intelligent Vehicles Symposium (IV), 2024<br /> 
                                        </span>
                                        <a href="https://ieeexplore.ieee.org/abstract/document/10588514/">Paper</a>
                                      </li>
                                      
                                </ol>
                                <br>
                                <h4 class="mb-4 text-uppercase">Journal</h4>
                                <ol class="journal">
                                  <li>
                                    <span class="text-primary"><b>Random bridge generator as a platform for developing computer vision-based structural inspection algorithms</b></span> 
                                    <br />
                                    <span class="text-500">
                                      Haojia Cheng*, <b>Wenhao Chai</b>*, Jiabao Hu*, Wenhao Ruan*, Mingyu Shi, Hyunjun Kim, Yifan Cao, Yasutaka Narazaki<sup>&Dagger;</sup><br />
                                    </span>
                                    <span class="text-900">
                                      Journal of Infrastructure Intelligence and Resilience<br /> 
                                    </span>
                                    <a href="https://www.sciencedirect.com/science/article/pii/S2772991524000173">Paper</a>
                                  </li>
                                  <hr>
                                  <li>
                                    <span class="text-primary"><b>Unsupervised Domain Adaptation Approach for Vision-based Semantic Understanding of Bridge Inspection Scenes without Manual Annotations</b></span> 
                                    <br />
                                    <span class="text-500">
                                      Yasutaka Narazaki<sup>&Dagger;</sup>, Wendong Pang, Gaoang Wang, <b>Wenhao Chai</b> <br /> 
                                    </span>
                                    <span class="text-900">  
                                      ACSE Journal of Bridge Engineering<br /> 
                                    </span>
                                    <a href="https://ascelibrary.org/doi/abs/10.1061/JBENF2.BEENG-6490">Paper</a>
                                  </li>
                                </ol>
                                <br>
                                <h4 class="mb-4 text-uppercase">Workshop</h4>
                                <ol class="workshop">
                                  <li>
                                    <span class="text-primary"><b>STEVE Series: Step-by-Step Construction of Agent Systems in Minecraft</b></span> 
                                    <br />
                                    <span class="text-500">
                                      Zhonghan Zhao*, <b>Wenhao Chai</b>*<sup>&dagger;</sup>, Xuan Wang, Ke Ma, Kewei Chen, Dongxu Guo, Tian Ye, Yanting Zhang, Hongwei Wang, Gaoang Wang<sup>&Dagger;</sup> <br /> 
                                    </span>
                                    <span class="text-900">
                                      Computer Vision and Pattern Recognition (CVPR) Workshop @ Embodied AI, 2024<br />
                                    </span>
                                    <a href="https://rese1f.github.io/STEVE/">Project Page</a>
                                    |
                                    <a href="https://arxiv.org/abs/2406.11247">Paper</a>
                                    |
                                    <a href="https://github.com/rese1f/STEVE">Dataset</a>
                                    |
                                    <a href="https://github.com/rese1f/STEVE">Code</a>
                                  </li>
                                  <hr>
                                  <li>
                                    <span class="text-primary"><b>NTIRE 2024 Image Shadow Removal Challenge Report</b></span> 
                                    <br />
                                    <span class="text-900">
                                      Computer Vision and Pattern Recognition (CVPR) Workshop @ NTIRE, 2024<br />
                                    </span>
                                    <a href="https://openaccess.thecvf.com/content/CVPR2024W/NTIRE/papers/Vasluianu_NTIRE_2024_Image_Shadow_Removal_Challenge_Report_CVPRW_2024_paper.pdf">Report</a>
                                    |
                                    <a href="https://cvlai.net/ntire/2024/">Challenge Page</a>
                                  </li>
                                  <hr>
                                  <li>
                                    <span class="text-primary"><b>Hierarchical Auto-Organizing System for Open-Ended Multi-Agent Navigation</b></span> 
                                    <br />
                                    <span class="text-500">
                                      Zhonghan Zhao*, Kewei Chen*, Dongxu Guo*, <b>Wenhao Chai</b><sup>&dagger;</sup>, Tian Ye, Yanting Zhang, Gaoang Wang<sup>&Dagger;</sup> <br /> 
                                    </span>
                                    <span class="text-900">
                                      International Conference on Learning Representations (ICLR) Workshop @ LLM Agents, 2024<br /> 
                                    </span>
                                    <a href="https://arxiv.org/abs/2403.08282">Paper</a>
                                  </li>
                                  <hr>
                                  <li>
                                    <span class="text-primary"><b>Efficient Domain Adaptation via Generative Prior for 3D Infant Pose Estimation</b></span> 
                                    <br />
                                    <span class="text-500">
                                      Zhuoran Zhou, Zhongyu Jiang, <b>Wenhao Chai</b>, Cheng-Yen Yang, Lei Li<sup>&Dagger;</sup>, Jenq-Neng Hwang <br /> 
                                    </span>
                                    <span class="text-900">
                                      Winter Conference on Applications of Computer Vision (WACV) Workshop @ Computer Vision with Small Data, 2024<br /> 
                                    </span>
                                    <a href="https://zhyjiang.github.io/ZeDO-proj/#infantZeDO">Project Page</a>
                                    |
                                    <a href="https://arxiv.org/abs/2311.12043">Paper</a>
                                    |
                                    <a href="https://github.com/ipl-uw/ZeDO-Release">Code</a>
                                  </li>
                                </ol>

                                <br>
                                <h4 class="mb-4 text-uppercase">Preprint</h4>
                                <ol class="informal">
                                  <li>
                                    <span class="text-primary"><b>MovieChat+: Question-aware Sparse Memory for Long Video Question Answering</b></span> 
                                    <br />
                                    <span class="text-500">
                                      Enxin Song*, <b>Wenhao Chai</b>*<sup>&dagger;</sup>, Tian Ye, Jenq-Neng Hwang, Xi Li, Gaoang Wang<sup>&Dagger;</sup> <br /> 
                                    </span>
                                    <span class="text-900">  
                                      arXiv Preprint.<br /> 
                                    </span>
                                    <a href="https://rese1f.github.io/MovieChat/">Project Page</a>
                                    |
                                    <a href="https://arxiv.org/abs/2404.17176">Paper</a>
                                    |
                                    <a href="https://mp.weixin.qq.com/s/OGI9kbH80sc9oIWWnntEIA">Blog</a>
                                    |
                                    <a href="https://www.youtube.com/watch?v=rNzCzflVqBU">Video</a>
                                    |
                                    <a href="https://huggingface.co/datasets/Enxin/MovieChat-1K-test">Dataset</a>
                                    |
                                    <a href="https://github.com/rese1f/MovieChat">Leaderboard</a>
                                    |
                                    <a href="https://github.com/rese1f/MovieChat">Code</a>
                                  </li>
                                  <hr>
                                  <li>
                                    <span class="text-primary"><b>SAMURAI: Adapting Segment Anything Model for Zero-Shot Visual Tracking with Motion-Aware Memory</b></span> 
                                    <br />
                                    <span class="text-500">
                                      Cheng-Yen Yang, Hsiang-Wei Huang, <b>Wenhao Chai</b>, Zhongyu Jiang, Jenq-Neng Hwang<sup>&Dagger;</sup> <br /> 
                                    </span>
                                    <span class="text-900">  
                                      arXiv Preprint.<br /> 
                                    </span>
                                    <a href="https://yangchris11.github.io/samurai/">Project Page</a>
                                    |
                                    <a href="https://arxiv.org/abs/2410.03051">Paper</a>
                                    |
                                    <a href="https://www.youtube.com/watch?v=pHq9eMVdvcA">Video</a>
                                    |
                                    <a href="https://drive.google.com/drive/folders/1ssiDmsC7mw5AiItYQG4poiR1JgRq305y">Raw Result</a>
                                    |
                                    <a href="https://github.com/yangchris11/samurai">Code</a>
                                  </li>
                                  <hr>
                                  <li>
                                    <span class="text-primary"><b>CityCraft: A Real Crafter for 3D City Generation</b></span> 
                                    <br />
                                    <span class="text-500">
                                      Jie Deng*, <b>Wenhao Chai*</b>, Junsheng Huang*, Zhonghan Zhao*, Qixuan Huang, Mingyan Gao, Jianshu Guo, Shengyu Hao, Wenhao Hu, Jenq-Neng Hwang, Xi Li, Gaoang Wang<sup>&Dagger;</sup> <br /> 
                                    </span>
                                    <span class="text-900"> 
                                      arXiv Preprint.<br /> 
                                    </span>
                                    <a href="https://arxiv.org/abs/2406.04983">Paper</a>
                                    |
                                    <a href="https://github.com/djFatNerd/CityCraft">Code</a>
                                  </li>
                                  <hr>
                                  <li>
                                    <span class="text-primary"><b>VersaT2I: Improving Text-to-Image Models with Versatile Reward</b></span> 
                                    <br />
                                    <span class="text-500">
                                      Jianshu Guo*, <b>Wenhao Chai</b>*<sup>&dagger;</sup>, Jie Deng*, Hsiang-Wei Huang, Tian Ye, Yichen Xu, Jiawei Zhang, Jenq-Neng Hwang, Gaoang Wang<sup>&Dagger;</sup> <br /> 
                                    </span>
                                    <span class="text-900">  
                                      arXiv Preprint.<br /> 
                                    </span>
                                    <a href="https://arxiv.org/abs/2403.18493">Paper</a>
                                  </li>
                                  <hr>
                                  <li>
                                    <span class="text-primary"><b>Do We Really Need a Complex Agent System? Distill Embodied Agent into a Single Model</b></span> 
                                    <br />
                                    <span class="text-500">
                                      Zhonghan Zhao, Ke Ma, <b>Wenhao Chai</b><sup>&dagger;</sup>, Xuan Wang, Kewei Chen, Dongxu Guo, Yanting Zhang, Hongwei Wang, Gaoang Wang<sup>&Dagger;</sup> <br /> 
                                    </span>
                                    <span class="text-900">  
                                      arXiv Preprint.<br /> 
                                    </span>
                                    <a href="https://arxiv.org/abs/2404.04619">Paper</a>
                                  </li>
                                </ol>
                            </div>                                                     
                        </div>                        
                    </div>
                </div>
            </section>
            <!-- End of Section -->
        </main>
        <!-- End of Main Content -->


        <!-- Footer -->
        <footer class="footer_p text-white" style="background-image: url(assets/img/favicons/logo.svg)">
          <div class="container d-flex h-100">
              <div class="row flex-grow-1">
                  <div class="col-lg-3 pt-3 ext-l bg-dark text-center text-lg-left">
                      <div class="d-flex flex-column h-100">
                          <div class="pt-5 pt-lg-8 pb-4">
                              <img src="assets/img/favicons/logo2.svg" alt="" width="188" class="mb-4" />
                              <p class="mb-4 mt-0 fs--1"><br />
                              Juanxi Tian <br>
                              HKBU<br />
                              <!-- Princeton, NJ 08544 -->
                              </p>
      
                              <p class="fs--1">
                              <span class="text-white"><i class="zmdi zmdi-email zmdi-hc-fw mr-1"></i>juanxitian1031 [at] gmail [dot] com</span><br />
                              <span class="text-white"><i class="zmdi zmdi-twitter zmdi-hc-fw mr-1"></i><a href="https://x.com/JuanxiTian" target="_blank" class="text-white">@Juanxi Tian</a></span></p>
                          </div>
                      </div>
                  </div>

                    <div class="col d-flex flex-column mb-2 mt-3 pl-lg-7">
                        <div class="row pt-5 pt-lg-8 pb-4 pb-lg-6">
                            <div class="col-6 col-lg-3">
                                <h6 class="mb-1 mb-lg-4 text-uppercase">Publications</h6>
                                <ul class="pt-2 mb-5 fw-light list-unstyled">
                                    <li class="my-1"><a href="research.html#overview" class="text-white">By Topic</a></li>
                                    <li class="my-1"><a href="publication_year.html" class="text-white">By Year</a></li>
                                </ul>
                            </div>
                            <div class="col-6 col-lg-3">
                                <h6 class="mb-1 mb-lg-4 text-uppercase">About</h6>
                                <ul class="pt-2 mb-5 fw-light list-unstyled">
                                    <li class="my-1"><a href="research.html" class="text-white">Our Research</a></li>
                                </ul>
                            </div>
                        </div>

                        <div class="mt-auto d-flex justify-content-between">
                            <span class="fs--3 fs-lg--2">&copy; Juanxi Tian, 2025 | Template from <a href="https://www.mmlab-ntu.com/" target="_blank">here</a></span>
                        </div>
                    </div>
                </div>
            </div>
        </footer>
        <!-- End of Footer -->

        <!-- Top Button -->
        <a id="back-to-top" href="publication_year.html#" class="btn btn-light btn-lg back-to-top" role="button"><i class="fas fa-chevron-up"></i></a>
        <!-- End of Top Button -->


        <!-- Core Javascripts -->
        <script src="assets/vendor/jquery/dist/jquery.min.js"></script>
        <script src="assets/vendor/popper.js/dist/umd/popper.min.js"></script>
        <script src="assets/vendor/bootstrap/dist/js/bootstrap.min.js"></script>
        <script src="assets/vendor/typed.js/lib/typed.min.js"></script>

        <!-- Vendor Javascripts -->
        <script src="assets/vendor/rellax/rellax.min.js"></script>
        <script src="assets/vendor/sticky-kit/dist/sticky-kit.min.js"></script>
        <script src="assets/vendor/imagesloaded/imagesloaded.pkgd.min.js"></script>
        <script src="assets/vendor/isotope-layout/dist/isotope.pkgd.min.js"></script>
        <script src="assets/vendor/isotope-packery/packery-mode.pkgd.min.js"></script>
        <script src="assets/vendor/aos/dist/aos.js"></script>

        <!-- Theme Javascripts -->
        <script src="assets/js/theme.js"></script>
        <script src="assets/js/top.js"></script>
    </body>
</html>
